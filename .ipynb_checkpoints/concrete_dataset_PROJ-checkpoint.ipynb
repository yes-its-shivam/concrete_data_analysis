{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Import the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7220/1882328982.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#import pandas as pd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'font'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font',size=14)\n",
    "sns.set(style='white')\n",
    "sns.set(style='whitegrid',color_codes=True)\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor)\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the CSV file into pandas dataframe\n",
    "df=pd.read_csv('concrete (1).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic eda\n",
    "def eda(df):\n",
    "    print('--------------------------------------------HEAD-----------------------------------------------')\n",
    "    print(df.head())\n",
    "    print('--------------------------------------------TAIL-----------------------------------------------')\n",
    "    print(df.tail())\n",
    "    print('--------------------------------------------SHAPE-----------------------------------------------')\n",
    "    print(df.shape)\n",
    "    print('--------------------------------------------IS_NULL_SUM-----------------------------------------------')\n",
    "    print(df.isnull().sum())\n",
    "    print('--------------------------------------------IS_NA_SUM-----------------------------------------------')\n",
    "    print(df.isna().sum())\n",
    "    print('--------------------------------------------COLUMNS-----------------------------------------------')\n",
    "    print(df.columns)\n",
    "    print('--------------------------------------------DTYPES-----------------------------------------------')\n",
    "    print(df.dtypes)\n",
    "    print('--------------------------------------------DESCRIBE-----------------------------------------------')\n",
    "    print(df.describe())\n",
    "    print('--------------------------------------------INFO-----------------------------------------------')\n",
    "    print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Exploratory data quality report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Univariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of independent attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Range of values observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--------------------------------------------CEMENT-----------------------------------------------')\n",
    "print('Range of values: ', df['cement'].max()-df['cement'].min())\n",
    "print('--------------------------------------------SLAG-----------------------------------------------')\n",
    "print('Range of values: ', df['slag'].max()-df['slag'].min())\n",
    "print('--------------------------------------------ASH-----------------------------------------------')\n",
    "print('Range of values: ', df['ash'].max()-df['ash'].min())\n",
    "print('--------------------------------------------WATER-----------------------------------------------')\n",
    "print('Range of values: ', df['water'].max()-df['water'].min())\n",
    "print('--------------------------------------------SUPERPLASTIC-----------------------------------------------')\n",
    "print('Range of values: ', df['superplastic'].max()-df['superplastic'].min())\n",
    "print('--------------------------------------------COARSEAGG-----------------------------------------------')\n",
    "print('Range of values: ', df['coarseagg'].max()-df['coarseagg'].min())\n",
    "print('--------------------------------------------FINEAGG-----------------------------------------------')\n",
    "print('Range of values: ', df['fineagg'].max()-df['fineagg'].min())\n",
    "print('--------------------------------------------AGE-----------------------------------------------')\n",
    "print('Range of values: ', df['age'].max()-df['age'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Central values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns[0:-1]:\n",
    "    print('---------------------------'+i+' VALUES'+'--------------------------------------')\n",
    "    Q1=df[i].quantile(q=0.25)\n",
    "    Q3=df[i].quantile(q=0.75)\n",
    "    print('1st Quartile (Q1) is: ', Q1)\n",
    "    print('3st Quartile (Q3) is: ', Q3)\n",
    "    print('Interquartile range (IQR) is ', stats.iqr(df[i]))\n",
    "    \n",
    "    # IQR=Q3-Q1\n",
    "    #lower 1.5*IQR whisker i.e Q1-1.5*IQR\n",
    "    #upper 1.5*IQR whisker i.e Q3+1.5*IQR\n",
    "    L_outliers=Q1-1.5*(Q3-Q1)\n",
    "    U_outliers=Q3+1.5*(Q3-Q1)\n",
    "    print('Lower outliers in cement: ', L_outliers)\n",
    "    print('Upper outliers in cement: ', U_outliers)\n",
    "    \n",
    "    print('Number of outliers in '+i+' upper : ', df[df[i]>U_outliers][i].count())\n",
    "    print('Number of outliers in '+i+' lower : ', df[df[i]<L_outliers][i].count())\n",
    "    print('% of Outlier in '+i+' upper: ',round(df[df[i]>U_outliers][i].count()*100/len(df)), '%')\n",
    "    print('% of Outlier in '+i+' lower: ',round(df[df[i]<L_outliers][i].count()*100/len(df)), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns[0:-1]:\n",
    "    fig, (ax1,ax2,ax3)=plt.subplots(1,3,figsize=(13,5))\n",
    "    #boxplot\n",
    "    sns.boxplot(x=i,data=df,orient='v',ax=ax1)\n",
    "    ax1.set_ylabel(i, fontsize=15)\n",
    "    ax1.set_title('Distribution of'+i, fontsize=15)\n",
    "    ax1.tick_params(labelsize=15)\n",
    "\n",
    "    #distplot\n",
    "    sns.distplot(df[i],ax=ax2)\n",
    "    ax2.set_xlabel(i, fontsize=15)\n",
    "    ax2.set_ylabel('Strength', fontsize=15)\n",
    "    ax2.set_title(i+' vs Strength', fontsize=15)\n",
    "    ax2.tick_params(labelsize=15)\n",
    "\n",
    "    #histogram\n",
    "    ax3.hist(df[i])\n",
    "    ax3.set_xlabel(i, fontsize=15)\n",
    "    ax3.set_ylabel('Strength', fontsize=15)\n",
    "    ax3.set_title(i+' vs Strength', fontsize=15)\n",
    "    ax3.tick_params(labelsize=15)\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.5)\n",
    "    plt.tight_layout() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from boxplot We can see observe that :\n",
    "- distribution of outliers\n",
    "- quartile range\n",
    "\n",
    "from distplot and histogram We can see observe that :\n",
    "- cement is almost normal. \n",
    "- slag has  three gausssians and rightly skewed.\n",
    "- ash has two gaussians and rightly skewed.\n",
    "- water has three guassians and slighly left skewed.\n",
    "- superplastic has two gaussians and rightly skewed.\n",
    "- coarseagg has three guassians and almost normal.\n",
    "- fineagg has almost two guassians and looks like normal.\n",
    "- age has multiple guassians and rightly skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Range of values observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    print('---------------'+i+'-----------------')\n",
    "    print('Range of values: ', df[i].max()-df[i].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Multivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram \n",
    "df.hist(figsize=(15,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is also giving the same information like distance plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pairplot- plot density curve instead of histogram in diagonal\n",
    "sns.pairplot(df, diag_kind='kde')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagonals Analysis\n",
    "The diagonal gives the same information, we got using distplot.\n",
    "* cement attribute have almost normal curve.\n",
    "* slag has  two gausssians and rightly skewed.It shows the presence of outlies.\n",
    "* ash has two gaussians and rightly skewed.It shows the presence of outlies.\n",
    "* water has atleast guassians and slighly left skewed.It shows the presence of outlies.\n",
    "* superplastic has multiple gaussians and rightly skewed.It shows the presence of outlies.\n",
    "* coarseagg has three guassians and almost normal.\n",
    "* fineagg has almost two guassians and looks like normal.\n",
    "* age has multiple guassians and rightly skewed. It shows the presence of outlies.\n",
    "* strength is close to a normal curve.\n",
    "\n",
    "We not only have missing values problem but also outliers problem in the dataset.\n",
    "\n",
    "#### Off Diagonal Analysis: Relationship between indpendent attributes\n",
    "##### Scatter plots\n",
    "- cement vs other independent attributes: This attribute does not have any significant relation with slag, ash, water, superplatic, coarseagg,fineagg and age. It almost spread like a cloud. If we had calculated the r value it would have come close to 0.\n",
    "- slag vs other independent attributes: This attribute also does not have any significant relation with ash, water, superplatic, coarseagg,fineagg and age. It almost spread like a cloud. If we had calculated the r value it would have come close to 0.\n",
    "- ash vs other independent attributes: This attribute also does not have any significant relation with water, superplatic, coarseagg,fineagg and age. It almost spread like a cloud. If we had calculated the r value it would have come close to 0.\n",
    "- water vs other independent attributes: This attribute have negative linear relationship with superplastic and fineagg. It does not have any significant relationship with other independent atributes. This is true  as Superplasticizers allows the reduction of water in the concrete upto the extent of 30% without reducing the workability.\n",
    "- superplastic vs other independent attributes:This attribute have negative linear relationship with water only. It does not have any significant relationship with other independent attributes.\n",
    "- coarseagg vs other independent attributes:This attribute also does not have any significant relation with any other attributes. It almost spread like a cloud. If we had calculated the r value it would have come close to 0.\n",
    "- fineagg vs other independent attributes:It has negative linear relationship with water. It does not have any significant relation with any other attributes. It almost spread like a cloud. If we had calculated the r value it would have come close to 0.\n",
    "\n",
    "\n",
    "#### strength attribute : Relationship between dependent and independent attributes\n",
    "strength: Now its comparing the target column with all other independent attributes and its showing us very vital information.\n",
    "- strength vs cement: It is linearly related to the cement. The relationship is positive and we can see that for a given value of cement we have a multiple values of strength. Which one should we pick we don't know. Hence Cement though it has poditive relationship with the strength, it is not a very good predictor. It is a weak predictor.\n",
    "- strength vs slag: There is no particular trend.\n",
    "- strength vs ash: There is also no particular trend.\n",
    "- strength vs age: For a given value of age, we have different values of strength. Hence, It is not a good predictor.\n",
    "- strength vs superplastic:For a given value of age, we have different values of strength. Hence, It is not a good predictor.\n",
    "- Other attributes does not give any strong relationship with strength.\n",
    "\n",
    "Hence, we can see that none of the independent attributes are a good predictors of the strength attribute. There is a no linear relationship between them.\n",
    "\n",
    "So, we will not use Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix \n",
    "cor=df.corr()\n",
    "cor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here, we can see the correlation value between the attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heatmap\n",
    "sns.set(font_scale=1.15)\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "sns.heatmap(cor, vmax=.8, linewidths=0.01,\n",
    "            square=True,annot=True,cmap=\"BuPu\",linecolor=\"black\")\n",
    "plt.title('Correlation between features');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is also giving the same information we observed in pairplot analysis. \n",
    "* water shows significant negative relationship with superplastic and fineagg. It also shows some kind of positive relationship with slag and age.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lm plot\n",
    "for i in df.columns:\n",
    "    for j in df.columns:\n",
    "        if i == j:\n",
    "            break\n",
    "        else:\n",
    "            sns.lmplot(x=i,y=j,data=df)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Strategies to handle different data challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can see that there are no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating copy of original dataset\n",
    "df1=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again check for outliers in dataset after handling missing values using boxplot\n",
    "df1.boxplot(figsize=(35,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It also shows that slag, ash, water superplastic, and age contains outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of outliers present in the dataset\n",
    "print('Number of outliers in cement: ',df1[((df1.cement - df1.cement.mean()) / df1.cement.std()).abs() >3]['cement'].count())\n",
    "print('Number of outliers in slag: ',df1[((df1.slag - df1.slag.mean()) / df1.slag.std()).abs() >3]['slag'].count())\n",
    "print('Number of outliers in ash: ',df1[((df1.ash - df1.ash.mean()) / df1.ash.std()).abs() >3]['ash'].count())\n",
    "print('Number of outliers in water: ',df1[((df1.water - df1.water.mean()) / df1.water.std()).abs() >3]['water'].count())\n",
    "print('Number of outliers in superplastic: ',df1[((df1.superplastic - df1.superplastic.mean()) / df1.superplastic.std()).abs() >3]['superplastic'].count())\n",
    "print('Number of outliers in coarseagg: ',df1[((df1.coarseagg - df1.coarseagg.mean()) / df1.coarseagg.std()).abs() >3]['coarseagg'].count())\n",
    "print('Number of outliers in fineagg: ',df1[((df1.fineagg - df1.fineagg.mean()) / df1.fineagg.std()).abs() >3]['fineagg'].count())\n",
    "print('Number of outliers in age: ',df1[((df1.age - df1.age.mean()) / df1.age.std()).abs() >3]['age'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here, we have used Standard deviation method to detect the outliers.If we have any data point that is more than 3 times the standard deviation, then those points are very likely to be outliers.\n",
    "* We can see that slag, water, superplastic and age contain outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Records which contains the outliers in slag attribute\n",
    "print('Records containing outliers in slag: \\n',df1[((df1.slag - df1.slag.mean()) / df1.slag.std()).abs() >3]['slag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Records which contains the outliers in water attribute\n",
    "print('Records containing outliers in water: \\n',df1[((df1.water - df1.water.mean()) / df1.water.std()).abs() >3]['water'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Records which contains the outliers in superplastic attribute\n",
    "print('Records containing outliers in superplastic: \\n',df1[((df1.superplastic - df1.superplastic.mean()) / df1.superplastic.std()).abs() >3]['superplastic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Records which contains the outliers in age attribute\n",
    "print('Records containing outliers in age: \\n',df1[((df1.age - df1.age.mean()) / df1.age.std()).abs() >3]['age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the outliers by median\n",
    "for col_name in df1.columns[:-1]:\n",
    "    q1 = df1[col_name].quantile(0.25)\n",
    "    q3 = df1[col_name].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    low = q1-1.5*iqr\n",
    "    high = q3+1.5*iqr\n",
    "    df1.loc[(df1[col_name] < low) | (df1[col_name] > high), col_name] = df1[col_name].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again check for outliers in dataset using boxplot\n",
    "df1.boxplot(figsize=(35,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of outliers present in the dataset\n",
    "print('Number of outliers in cement: ',df1[((df1.cement - df1.cement.mean()) / df1.cement.std()).abs() >3]['cement'].count())\n",
    "print('Number of outliers in slag: ',df1[((df1.slag - df1.slag.mean()) / df1.slag.std()).abs() >3]['slag'].count())\n",
    "print('Number of outliers in ash: ',df1[((df1.ash - df1.ash.mean()) / df1.ash.std()).abs() >3]['ash'].count())\n",
    "print('Number of outliers in water: ',df1[((df1.water - df1.water.mean()) / df1.water.std()).abs() >3]['water'].count())\n",
    "print('Number of outliers in superplastic: ',df1[((df1.superplastic - df1.superplastic.mean()) / df1.superplastic.std()).abs() >3]['superplastic'].count())\n",
    "print('Number of outliers in coarseagg: ',df1[((df1.coarseagg - df1.coarseagg.mean()) / df1.coarseagg.std()).abs() >3]['coarseagg'].count())\n",
    "print('Number of outliers in fineagg: ',df1[((df1.fineagg - df1.fineagg.mean()) / df1.fineagg.std()).abs() >3]['fineagg'].count())\n",
    "print('Number of outliers in age: ',df1[((df1.age - df1.age.mean()) / df1.age.std()).abs() >3]['age'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the dataset\n",
    "df_z = df1.apply(zscore)\n",
    "df_z=pd.DataFrame(df_z,columns=df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here, all the attributes in the same scale(unit) except the age attribute. Hence, we are scaling the attributes. We are using zscore for scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into independent and dependent attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#independent and dependent variables\n",
    "X=df_z.iloc[:,0:8]\n",
    "y = df_z.iloc[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into three sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X and y into training and test set in 70:30 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Building\n",
    "# 4. Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecisionTree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeRegressor()\n",
    "model.fit(X_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing the feature importance\n",
    "print('Feature importances: \\n',pd.DataFrame(model.feature_importances_,columns=['Imp'],index=X_train.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So, cement, age and water are significant attributes.\n",
    "* Here, ash, coarseagg, fineagg, superplastic and slag are the less significant variable.These will impact less to the strength column. This we have seen in pairplot also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "# performance on train data\n",
    "print('Performance on training data using DT:',model.score(X_train,y_train))\n",
    "# performance on test data\n",
    "print('Performance on testing data using DT:',model.score(X_test,y_test))\n",
    "#Evaluate the model using accuracy\n",
    "acc_DT=metrics.r2_score(y_test, y_pred)\n",
    "print('Accuracy DT: ',acc_DT)\n",
    "print('MSE: ',metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There is a overfitting in the model as the dataset is performing 99% accurately in trainnig data. However, the accuracy on test data drops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the accuracy results for each model in a dataframe for final comparison\n",
    "results = pd.DataFrame({'Method':['Decision Tree'], 'accuracy': acc_DT},index={'1'})\n",
    "results = results[['Method', 'accuracy']]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 10\n",
    "seed = 77\n",
    "kfold = KFold(n_splits=num_folds, random_state=None)\n",
    "results1 = cross_val_score(model,X, y, cv=kfold)\n",
    "accuracy=np.mean(abs(results1))\n",
    "print('Average accuracy: ',accuracy)\n",
    "print('Standard Deviation: ',results1.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the accuracy results for each model in a dataframe for final comparison\n",
    "tempResultsDf = pd.DataFrame({'Method':['Decision Tree k fold'], 'accuracy': [accuracy]},index={'2'})\n",
    "results = pd.concat([results, tempResultsDf])\n",
    "results = results[['Method', 'accuracy']]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the least significant variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_z.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a copy of the dataset\n",
    "df2=df_z.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#independent and dependent variable\n",
    "X = df2.drop( ['strength','ash','coarseagg','fineagg'] , axis=1)\n",
    "y = df2['strength']\n",
    "# Split X and y into training and test set in 70:30 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = DecisionTreeRegressor()\n",
    "dt_model.fit(X_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing the feature importance\n",
    "print('Feature importances: \\n',pd.DataFrame(dt_model.feature_importances_,columns=['Imp'],index=X_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dt_model.predict(X_test)\n",
    "# performance on train data\n",
    "print('Performance on training data using DT:',dt_model.score(X_train,y_train))\n",
    "# performance on test data\n",
    "print('Performance on testing data using DT:',dt_model.score(X_test,y_test))\n",
    "#Evaluate the model using accuracy\n",
    "acc_DT=metrics.r2_score(y_test, y_pred)\n",
    "print('Accuracy DT: ',acc_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the accuracy results for each model in a dataframe for final comparison\n",
    "tempResultsDf = pd.DataFrame({'Method':['Decision Tree2'], 'accuracy': [acc_DT]},index={'3'})\n",
    "results = pd.concat([results, tempResultsDf])\n",
    "results = results[['Method', 'accuracy']]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The acuracy on testing dataset is not improved, still it is an overfit model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculating value of GINI IMPURITY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(y):\n",
    "    # calculate gini_impurity given labels/classes of each example\n",
    "    m = y.shape[0]\n",
    "    cnts = dict(zip(*np.unique(y, return_counts = True)))\n",
    "    impurity = 1 - sum((cnt/m)**2 for cnt in cnts.values())\n",
    "    return impurity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_impurity(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* small value of gini index means data in decision tree is highly unordered therefore\n",
    "  problem with this model is poor ordering of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning of Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#independent and dependent variables\n",
    "X=df_z.iloc[:,0:8]\n",
    "y = df_z.iloc[:,8]\n",
    "# Split X and y into training and test set in 70:30 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularizing the Decision tree classifier and fitting the model\n",
    "reg_dt_model = DecisionTreeRegressor( max_depth = 4,random_state=1,min_samples_leaf=5)\n",
    "reg_dt_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (pd.DataFrame(reg_dt_model.feature_importances_, columns = [\"Imp\"], index = X_train.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here, we can see that ash,coarseagg and fineagg are least significant variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 10\n",
    "seed = 77\n",
    "kfold = KFold(n_splits=num_folds, random_state=None)\n",
    "results1 = cross_val_score(reg_dt_model,X, y, cv=kfold)\n",
    "accuracy=np.mean(abs(results1))\n",
    "print('Average accuracy: ',accuracy)\n",
    "print('Standard Deviation: ',results1.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the accuracy results for each model in a dataframe for final comparison\n",
    "tempResultsDf = pd.DataFrame({'Method':['Pruned Decision Tree k fold'], 'accuracy': [accuracy]},index={'5'})\n",
    "results = pd.concat([results, tempResultsDf])\n",
    "results = results[['Method', 'accuracy']]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a copy of the dataset\n",
    "df3=df_z.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#independent and dependent variable\n",
    "X = df3.drop( ['strength','ash','coarseagg','fineagg'] , axis=1)\n",
    "y = df3['strength']\n",
    "# Split X and y into training and test set in 70:30 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularizing the Decision tree classifier and fitting the model\n",
    "reg_dt_model = DecisionTreeRegressor( max_depth = 4,random_state=1,min_samples_leaf=5)\n",
    "reg_dt_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reg_dt_model.predict(X_test)\n",
    "# performance on train data\n",
    "print('Performance on training data using DT:',reg_dt_model.score(X_train,y_train))\n",
    "# performance on test data\n",
    "print('Performance on testing data using DT:',reg_dt_model.score(X_test,y_test))\n",
    "#Evaluate the model using accuracy\n",
    "acc_RDT=metrics.r2_score(y_test, y_pred)\n",
    "print('Accuracy DT: ',acc_RDT)\n",
    "print('MSE: ',metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the accuracy results for each model in a dataframe for final comparison\n",
    "tempResultsDf = pd.DataFrame({'Method':['Pruned Decision Tree2'], 'accuracy': [acc_RDT]},index={'6'})\n",
    "results = pd.concat([results, tempResultsDf])\n",
    "results = results[['Method', 'accuracy']]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=RandomForestRegressor()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "# performance on train data\n",
    "print('Performance on training data using RFR:',model.score(X_train,y_train))\n",
    "# performance on test data\n",
    "print('Performance on testing data using RFR:',model.score(X_test,y_test))\n",
    "#Evaluate the model using accuracy\n",
    "acc_RFR=metrics.r2_score(y_test, y_pred)\n",
    "print('Accuracy DT: ',acc_RFR)\n",
    "print('MSE: ',metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This model is also overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the accuracy results for each model in a dataframe for final comparison\n",
    "tempResultsDf = pd.DataFrame({'Method':['Random Forest Regressor'], 'accuracy': [acc_RFR]},index={'7'})\n",
    "results = pd.concat([results, tempResultsDf])\n",
    "results = results[['Method', 'accuracy']]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 10\n",
    "seed = 77\n",
    "kfold = KFold(n_splits=num_folds, random_state=None)\n",
    "results1 = cross_val_score(model,X, y, cv=kfold)\n",
    "accuracy=np.mean(abs(results1))\n",
    "print('Average accuracy: ',accuracy)\n",
    "print('Standard Deviation: ',results1.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the accuracy results for each model in a dataframe for final comparison\n",
    "tempResultsDf = pd.DataFrame({'Method':['Random Forest Regressor k fold'], 'accuracy': [accuracy]},index={'8'})\n",
    "results = pd.concat([results, tempResultsDf])\n",
    "results = results[['Method', 'accuracy']]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=GradientBoostingRegressor()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "# performance on train data\n",
    "print('Performance on training data using GBR:',model.score(X_train,y_train))\n",
    "# performance on test data\n",
    "print('Performance on testing data using GBR:',model.score(X_test,y_test))\n",
    "#Evaluate the model using accuracy\n",
    "acc_GBR=metrics.r2_score(y_test, y_pred)\n",
    "print('Accuracy DT: ',acc_GBR)\n",
    "print('MSE: ',metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the accuracy results for each model in a dataframe for final comparison\n",
    "tempResultsDf = pd.DataFrame({'Method':['Gradient Boost Regressor'], 'accuracy': [acc_GBR]},index={'9'})\n",
    "results = pd.concat([results, tempResultsDf])\n",
    "results = results[['Method', 'accuracy']]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 10\n",
    "seed = 77\n",
    "kfold = KFold(n_splits=num_folds, random_state=None)\n",
    "results1 = cross_val_score(model,X, y, cv=kfold)\n",
    "accuracy=np.mean(abs(results1))\n",
    "print('Average accuracy: ',accuracy)\n",
    "print('Standard Deviation: ',results1.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the accuracy results for each model in a dataframe for final comparison\n",
    "tempResultsDf = pd.DataFrame({'Method':['Gradient Boost Regressor k fold'], 'accuracy': [accuracy]},index={'10'})\n",
    "results = pd.concat([results, tempResultsDf])\n",
    "results = results[['Method', 'accuracy']]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error=[]\n",
    "for i in range(1,30):\n",
    "    knn = KNeighborsRegressor(n_neighbors=i)\n",
    "    knn.fit(X_train,y_train)\n",
    "    pred_i = knn.predict(X_test)\n",
    "    error.append(np.mean(pred_i!=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(range(1,30),error,color='red', linestyle='dashed',marker='o',markerfacecolor='blue',markersize=10)\n",
    "plt.title('Error Rate K Value')\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('Mean error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k=3\n",
    "model = KNeighborsRegressor(n_neighbors=3)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "# performance on train data\n",
    "print('Performance on training data using KNNR:',model.score(X_train,y_train))\n",
    "# performance on test data\n",
    "print('Performance on testing data using KNNR:',model.score(X_test,y_test))\n",
    "#Evaluate the model using accuracy\n",
    "acc_K=metrics.r2_score(y_test, y_pred)\n",
    "print('Accuracy KNNR: ',acc_K)\n",
    "print('MSE: ',metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the accuracy results for each model in a dataframe for final comparison\n",
    "tempResultsDf = pd.DataFrame({'Method':['KNN Regressor'], 'accuracy': [acc_K]},index={'15'})\n",
    "results = pd.concat([results, tempResultsDf])\n",
    "results = results[['Method', 'accuracy']]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 10\n",
    "seed = 77\n",
    "kfold = KFold(n_splits=num_folds, random_state=None)\n",
    "results1 = cross_val_score(model,X, y, cv=kfold)\n",
    "accuracy=np.mean(abs(results1))\n",
    "print('Average accuracy: ',accuracy)\n",
    "print('Standard Deviation: ',results1.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the accuracy results for each model in a dataframe for final comparison\n",
    "tempResultsDf = pd.DataFrame({'Method':['KNN Regressor k fold'], 'accuracy': [accuracy]},index={'16'})\n",
    "results = pd.concat([results, tempResultsDf])\n",
    "results = results[['Method', 'accuracy']]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVR(kernel='linear')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "# performance on train data\n",
    "print('Performance on training data using SVR:',model.score(X_train,y_train))\n",
    "# performance on test data\n",
    "print('Performance on testing data using SVR:',model.score(X_test,y_test))\n",
    "#Evaluate the model using accuracy\n",
    "acc_S=metrics.r2_score(y_test, y_pred)\n",
    "print('Accuracy SVR: ',acc_S)\n",
    "print('MSE: ',metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the accuracy results for each model in a dataframe for final comparison\n",
    "tempResultsDf = pd.DataFrame({'Method':['Support Vector Regressor'], 'accuracy': [acc_S]},index={'17'})\n",
    "results = pd.concat([results, tempResultsDf])\n",
    "results = results[['Method', 'accuracy']]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 10\n",
    "seed = 77\n",
    "kfold = KFold(n_splits=num_folds, random_state=None)\n",
    "results1 = cross_val_score(model,X, y, cv=kfold)\n",
    "accuracy=np.mean(abs(results1))\n",
    "print('Average accuracy: ',accuracy)\n",
    "print('Standard Deviation: ',results1.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the accuracy results for each model in a dataframe for final comparison\n",
    "tempResultsDf = pd.DataFrame({'Method':['SVR k fold'], 'accuracy': [accuracy]},index={'18'})\n",
    "results = pd.concat([results, tempResultsDf])\n",
    "results = results[['Method', 'accuracy']]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemeble KNN Regressor, SVR, LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiple model Ensemble\n",
    "from sklearn import svm\n",
    "LR=LinearRegression()\n",
    "KN=KNeighborsRegressor(n_neighbors=3)\n",
    "SVM=svm.SVR(kernel='linear') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evc=VotingRegressor(estimators=[('LR',LR),('KN',KN),('SVM',SVM)])\n",
    "evc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = evc.predict(X_test)\n",
    "# performance on train data\n",
    "print('Performance on training data using ensemble:',evc.score(X_train,y_train))\n",
    "# performance on test data\n",
    "print('Performance on testing data using ensemble:',evc.score(X_test,y_test))\n",
    "#Evaluate the model using accuracy\n",
    "acc_E=metrics.r2_score(y_test, y_pred)\n",
    "print('Accuracy ensemble: ',acc_E)\n",
    "print('MSE: ',metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the accuracy results for each model in a dataframe for final comparison\n",
    "tempResultsDf = pd.DataFrame({'Method':['Ensemble'], 'accuracy': [acc_E]},index={'19'})\n",
    "results = pd.concat([results, tempResultsDf])\n",
    "results = results[['Method', 'accuracy']]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 10\n",
    "seed = 77\n",
    "kfold = KFold(n_splits=num_folds, random_state=None)\n",
    "results1 = cross_val_score(evc,X, y, cv=kfold)\n",
    "accuracy=np.mean(abs(results1))\n",
    "print('Average accuracy: ',accuracy)\n",
    "print('Standard Deviation: ',results1.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the accuracy results for each model in a dataframe for final comparison\n",
    "tempResultsDf = pd.DataFrame({'Method':['Ensemble k fold'], 'accuracy': [accuracy]},index={'20'})\n",
    "results = pd.concat([results, tempResultsDf])\n",
    "results = results[['Method', 'accuracy']]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After applying all the models we can see that Random Forest Regressor, Random Forest Regressor k fold, Gradient Boost Regressor, Gradient Boost Regressor k fold are giving better results as compared to other models.\n",
    "* Now as the dataset have different gaussians, we can apply k means clustering and then we can apply the models and compare the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_XY = X.join(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = concrete_XY.values\n",
    "# Number of bootstrap samples to create\n",
    "n_iterations = 1000        \n",
    "# size of a bootstrap sample\n",
    "n_size = int(len(df_z) * 1)    \n",
    "\n",
    "# run bootstrap\n",
    "# empty list that will hold the scores for each bootstrap iteration\n",
    "stats = list()   \n",
    "for i in range(n_iterations):\n",
    "    # prepare train and test sets\n",
    "    train = resample(values, n_samples=n_size)  # Sampling with replacement \n",
    "    test = np.array([x for x in values if x.tolist() not in train.tolist()])  # picking rest of the data not considered in sample\n",
    "    \n",
    "    \n",
    "     # fit model\n",
    "    gbmTree = GradientBoostingRegressor(n_estimators=50)\n",
    "    # fit against independent variables and corresponding target values\n",
    "    gbmTree.fit(train[:,:-1], train[:,-1]) \n",
    "    # Take the target column for all rows in test set\n",
    "\n",
    "    y_test = test[:,-1]    \n",
    "    # evaluate model\n",
    "    # predict based on independent variables in the test data\n",
    "    score = gbmTree.score(test[:, :-1] , y_test)\n",
    "    predictions = gbmTree.predict(test[:, :-1])  \n",
    "\n",
    "    stats.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = concrete_XY.values\n",
    "# Number of bootstrap samples to create\n",
    "n_iterations = 1000        \n",
    "# size of a bootstrap sample\n",
    "n_size = int(len(df_z) * 1)    \n",
    "\n",
    "# run bootstrap\n",
    "# empty list that will hold the scores for each bootstrap iteration\n",
    "stats = list()   \n",
    "for i in range(n_iterations):\n",
    "    # prepare train and test sets\n",
    "    train = resample(values, n_samples=n_size)  # Sampling with replacement \n",
    "    test = np.array([x for x in values if x.tolist() not in train.tolist()])  # picking rest of the data not considered in sample\n",
    "    \n",
    "    \n",
    "     # fit model\n",
    "    rfTree = RandomForestRegressor(n_estimators=100)\n",
    "    # fit against independent variables and corresponding target values\n",
    "    rfTree.fit(train[:,:-1], train[:,-1]) \n",
    "    # Take the target column for all rows in test set\n",
    "\n",
    "    y_test = test[:,-1]    \n",
    "    # evaluate model\n",
    "    # predict based on independent variables in the test data\n",
    "    score = rfTree.score(test[:, :-1] , y_test)\n",
    "    predictions = rfTree.predict(test[:, :-1])  \n",
    "\n",
    "    stats.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model performance range at 95% confidence level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot scores\n",
    "\n",
    "from matplotlib import pyplot\n",
    "pyplot.hist(stats)\n",
    "pyplot.show()\n",
    "# confidence intervals\n",
    "alpha = 0.95                             # for 95% confidence \n",
    "p = ((1.0-alpha)/2.0) * 100              # tail regions on right and left .25 on each side indicated by P value (border)\n",
    "lower = max(0.0, np.percentile(stats, p))  \n",
    "p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "upper = min(1.0, np.percentile(stats, p))\n",
    "print('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bootstrap random forest  classification model performance is between 84.6%-90.5% which is better than other classification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
